{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flappy Bird is one of the most popular and iconic mobile game ever, with its massive popularity boom back in 2014. The game had everyone stuck to their phone screens tapping away for hours trying to set new high scores. The objective of the game was simple, to survive as long as possible without hitting the obstacles. Doing this however was not so simple. Getting a high score in flappy would take not only a huge amount of focus, but also a lot of time. However, because of its simplicity, and the fact that the game can go on forever, it seems like the perfect game to test out the limits of AI. Our project is to train an AI model using reinforcement learning to learn how to play the game and achieve the highest score possible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unautomated base game of flappy bird was taken from LeonMarqs's repository  https://github.com/LeonMarqs/Flappy-bird-python/tree/master, who credited  https://github.com/zhaolingzhi/FlapPyBird-master for the game_assets such as the images and sounds. We decided to use an external repository for the game, so that we would not have to build flappy bird from scratch. The goal of the project was not to build a game, but to automate the work of playing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were several parts of creating a deep Q network for flappy bird\n",
    "1. Setting up a working game environment such that it would be compatible with the agent, and provide the game state on request\n",
    "2. Defining the constraints and parameters of the main and target network\n",
    "3. Setting up the agent, so that it would be able to interact with the game environment by either jumping or not jumping on command\n",
    "4. Training the networks, by having the agent play the game and using a mix of exploration and exploitation to provide new data  \n",
    "5. Testing the network and evaluating results through use of a purely exploratory stage\n",
    "6. Finetuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Setup & Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model of the network we had\n",
    "* The input layer, taking in 4 inputs (bird_height, pipe_x_pos, top_pipe_y_pos, bottom_pipe_y_pos)\n",
    "* 2 Fully connected layers, both of 32 nodes\n",
    "* The output layer, with 2 nodes indicating whether or not to jump\n",
    "\t\n",
    "The data was taken from the game at every tick and fed through the network, alongside the action and reward that was observed. The network used 2 relu functions, a mean-squared error loss function, and an adam optimiser to exploit for it's speed. \n",
    "\n",
    "Other concepts included the use of a modified q value calculator : rewards + (1 - dones) * self.gamma for the update of future discounted rewards, taken from https://medium.com/@www.seymour/training-an-ai-to-play-a-game-using-deep-reinforcement-learning-b63534cfdecd. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and/or Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The members of our group were all fairly new to machine learning, hence we did a lot of research into various methods and implementations to compensate for our lack in knowledge. Admittedly we started off undirected and were overwhelmed by the concept of deep q learning, with the strategy of tackling problems that came our way one by one. \n",
    "\n",
    "Thanks to recommendation made by the tutor after our first meeting, we had a more cohesive understanding of Deep Q learning, and we directed focus into researching what our tutor had mentioned, such as double and duelling DQN's for environments.\n",
    "\n",
    "However since we had already built a foundation at that point, we wanted to continue testing our hypothesis. From our understanding double and duelling DQN's would make training faster by reducing the risk of overestimation of q values, however there was no research that indicated that these models would always excel over the simple DQN that we had already started working on. Because of our inexperience, and late head start (due to random group formation) we simply focused on getting a model working, and trivialised training time. We assumed that by passing in larger amounts of data for training, we could reduce the complexity of the model and still achieve a decent accuracy. Additionally, there was much more documentation for simple DQNs on the internet that we could use to help gauge the correctness of our model implementation, hence such is the reasoning behind us choosing a simple DQN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import time\n",
    "from pygame.locals import *\n",
    "\n",
    "# VARIABLES\n",
    "SCREEN_WIDTH = 400\n",
    "SCREEN_HEIGHT = 600\n",
    "SPEED = 10\n",
    "GRAVITY = 1\n",
    "GAME_SPEED = 10\n",
    "\n",
    "GROUND_WIDTH = 2 * SCREEN_WIDTH\n",
    "GROUND_HEIGHT = 100\n",
    "\n",
    "PIPE_WIDTH = 80\n",
    "PIPE_HEIGHT = 500\n",
    "PIPE_GAP = 200\n",
    "\n",
    "wing = 'assets/audio/wing.wav'\n",
    "hit = 'assets/audio/hit.wav'\n",
    "\n",
    "def show_score(screen, score, font):\n",
    "    score_surface = font.render(f'Score: {score}', True, (255, 255, 255))\n",
    "    screen.blit(score_surface, (SCREEN_WIDTH - score_surface.get_width() - 10, 10))\n",
    "\n",
    "def game_over_screen(screen, score, score_font):\n",
    "    \"\"\"\n",
    "    Show a game-over screen with the final score and a message to restart.\n",
    "    \"\"\"\n",
    "   #Prompt information for rendering scores and restarting\n",
    "    score_text = score_font.render(f'Final Score: {score}', True, (255, 255, 255))\n",
    "    restart_text = score_font.render('Press Space to Restart', True, (255, 255, 255))\n",
    "\n",
    "   #Display score and prompt information in the center\n",
    "    score_rect = score_text.get_rect(center=(SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2))\n",
    "    restart_rect = restart_text.get_rect(center=(SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2 + 50))\n",
    "\n",
    "    screen.blit(score_text, score_rect)\n",
    "    screen.blit(restart_text, restart_rect)\n",
    "\n",
    "    #Update screen display\n",
    "    pygame.display.update()\n",
    "\n",
    "    #Waiting for player response\n",
    "    waiting = True\n",
    "    while waiting:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                return False  #End the game\n",
    "            if event.type == KEYDOWN:\n",
    "                if event.key == K_SPACE:\n",
    "                    return True  #Restart the game\n",
    "\n",
    "def get_random_pipes(xpos):\n",
    "    size = random.randint(100, 300)\n",
    "    pipe = Pipe(False, xpos, size)\n",
    "    pipe_inverted = Pipe(True, xpos, SCREEN_HEIGHT - size - PIPE_GAP)\n",
    "    return pipe, pipe_inverted\n",
    "\n",
    "class Bird(pygame.sprite.Sprite):\n",
    "\n",
    "    def __init__(self):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "\n",
    "        self.images =  [pygame.image.load('assets/sprites/bluebird-upflap.png').convert_alpha(),\n",
    "                        pygame.image.load('assets/sprites/bluebird-midflap.png').convert_alpha(),\n",
    "                        pygame.image.load('assets/sprites/bluebird-downflap.png').convert_alpha()]\n",
    "\n",
    "        self.speed = SPEED\n",
    "\n",
    "        self.current_image = 0\n",
    "        self.image = pygame.image.load('assets/sprites/bluebird-upflap.png').convert_alpha()\n",
    "        self.mask = pygame.mask.from_surface(self.image)\n",
    "\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect[0] = SCREEN_WIDTH / 6\n",
    "        self.rect[1] = SCREEN_HEIGHT / 2\n",
    "\n",
    "    def update(self):\n",
    "        self.current_image = (self.current_image + 1) % 3\n",
    "        self.image = self.images[self.current_image]\n",
    "        self.speed += GRAVITY\n",
    "\n",
    "        #UPDATE HEIGHT\n",
    "        self.rect[1] += self.speed\n",
    "\n",
    "    def bump(self):\n",
    "        self.speed = -SPEED\n",
    "\n",
    "    def begin(self):\n",
    "        self.current_image = (self.current_image + 1) % 3\n",
    "        self.image = self.images[self.current_image]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Pipe(pygame.sprite.Sprite):\n",
    "\n",
    "    def __init__(self, inverted, xpos, ysize):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "\n",
    "        self. image = pygame.image.load('assets/sprites/pipe-green.png').convert_alpha()\n",
    "        self.image = pygame.transform.scale(self.image, (PIPE_WIDTH, PIPE_HEIGHT))\n",
    "\n",
    "\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect[0] = xpos\n",
    "\n",
    "        if inverted:\n",
    "            self.image = pygame.transform.flip(self.image, False, True)\n",
    "            self.rect[1] = - (self.rect[3] - ysize)\n",
    "        else:\n",
    "            self.rect[1] = SCREEN_HEIGHT - ysize\n",
    "\n",
    "\n",
    "        self.mask = pygame.mask.from_surface(self.image)\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.rect[0] -= GAME_SPEED\n",
    "\n",
    "        \n",
    "\n",
    "class Ground(pygame.sprite.Sprite):\n",
    "    \n",
    "    def __init__(self, xpos):\n",
    "        pygame.sprite.Sprite.__init__(self)\n",
    "        self.image = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "        self.image = pygame.transform.scale(self.image, (GROUND_WIDTH, GROUND_HEIGHT))\n",
    "\n",
    "        self.mask = pygame.mask.from_surface(self.image)\n",
    "\n",
    "        self.rect = self.image.get_rect()\n",
    "        self.rect[0] = xpos\n",
    "        self.rect[1] = SCREEN_HEIGHT - GROUND_HEIGHT\n",
    "    def update(self):\n",
    "        self.rect[0] -= GAME_SPEED\n",
    "\n",
    "\n",
    "class FlappyBirdEnv:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "        pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "        self.BACKGROUND = pygame.image.load('assets/sprites/background-day.png').convert()\n",
    "        self.BACKGROUND = pygame.transform.scale(self.BACKGROUND, (SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "        self.wing_sound = pygame.mixer.Sound(wing)  #Preload sound\n",
    "        self.hit_sound = pygame.mixer.Sound(hit)\n",
    "\n",
    "        self.score_font = pygame.font.Font(None, 32)\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.score = 0\n",
    "        self.bird_group = pygame.sprite.Group()\n",
    "        self.bird = Bird()\n",
    "        self.ground_group = pygame.sprite.Group()\n",
    "        self.pipe_group = pygame.sprite.Group()\n",
    "\n",
    "    def reset(self):\n",
    "        self.score = 0\n",
    "        self.bird_group.remove()\n",
    "        self.pipe_group.remove()\n",
    "        self.ground_group.remove()\n",
    "\n",
    "        self.bird_group = pygame.sprite.Group()\n",
    "        self.bird = Bird()\n",
    "        self.bird_group.add(self.bird)\n",
    "\n",
    "        self.ground_group = pygame.sprite.Group()\n",
    "        for i in range(2):\n",
    "            ground = Ground(GROUND_WIDTH * i)\n",
    "            self.ground_group.add(ground)\n",
    "\n",
    "        self.pipe_group = pygame.sprite.Group()\n",
    "        for i in range(2):\n",
    "            pipes = get_random_pipes(SCREEN_WIDTH * i + 800)\n",
    "            self.pipe_group.add(pipes[0])\n",
    "            self.pipe_group.add(pipes[1])\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.clock.tick(50)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == QUIT:\n",
    "                pygame.quit()\n",
    "                return False  #End the game\n",
    "        \n",
    "        if action == 1:\n",
    "            self.bird.bump()\n",
    "            self.wing_sound.play()\n",
    "\n",
    "        self.screen.blit(self.BACKGROUND, (0, 0))\n",
    "\n",
    "        if is_off_screen(self.ground_group.sprites()[0]):\n",
    "            self.ground_group.remove(self.ground_group.sprites()[0])\n",
    "            new_ground = Ground(GROUND_WIDTH - 20)\n",
    "            self.ground_group.add(new_ground)\n",
    "\n",
    "        if is_off_screen(self.pipe_group.sprites()[0]):\n",
    "            self.pipe_group.remove(self.pipe_group.sprites()[0])\n",
    "            self.pipe_group.remove(self.pipe_group.sprites()[0])\n",
    "            self.score += 1\n",
    "            pipes = get_random_pipes(SCREEN_WIDTH * 2)\n",
    "            self.pipe_group.add(pipes[0])\n",
    "            self.pipe_group.add(pipes[1])\n",
    "\n",
    "        self.bird_group.update()\n",
    "        self.ground_group.update()\n",
    "        self.pipe_group.update()\n",
    "\n",
    "        self.bird_group.draw(self.screen)\n",
    "        self.pipe_group.draw(self.screen)\n",
    "        self.ground_group.draw(self.screen)\n",
    "\n",
    "        show_score(self.screen, self.score, self.score_font)\n",
    "\n",
    "        pygame.display.update()\n",
    "        \n",
    "        if (pygame.sprite.groupcollide(self.bird_group, self.ground_group, False, False, pygame.sprite.collide_mask) or\n",
    "                pygame.sprite.groupcollide(self.bird_group, self.pipe_group, False, False, pygame.sprite.collide_mask)):\n",
    "            self.hit_sound.play()\n",
    "            observation = self.get_observation(), -100, 1\n",
    "            print(self.score)\n",
    "            #self.reset()\n",
    "            time.sleep(1)  #Pause briefly to display collision effects\n",
    "            return observation\n",
    "        return self.get_observation(), 0, 0\n",
    "\n",
    "    def get_observation(self):\n",
    "        return automatic_play(self.bird, self.pipe_group)\n",
    "\n",
    "def automatic_play(bird, pipe_group):\n",
    "    bird_height = bird.rect[1]\n",
    "    if (pipe_group.sprites()[0].rect[0] < 0):\n",
    "        pipe_x_pos = pipe_group.sprites()[2].rect[0]\n",
    "        top_pipe_y_pos = pipe_group.sprites()[2].rect[1]\n",
    "        bottom_pipe_y_pos = pipe_group.sprites()[3].rect[1]\n",
    "    else:\n",
    "        pipe_x_pos = pipe_group.sprites()[0].rect[0]\n",
    "        top_pipe_y_pos = pipe_group.sprites()[0].rect[1]\n",
    "        bottom_pipe_y_pos = pipe_group.sprites()[1].rect[1]\n",
    "    return (bird_height, pipe_x_pos, top_pipe_y_pos, bottom_pipe_y_pos)\n",
    "\n",
    "def is_off_screen(sprite):\n",
    "    return sprite.rect[0] < -(sprite.rect[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from flappy_RI import FlappyBirdEnv\n",
    "import os\n",
    "\n",
    "################# QNET COMPLETE ######################\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, input_dim, f1_dim, f2_dim, output_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, f1_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(f1_dim, f2_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(f2_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "################# AGENT ######################\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.DQN = QNet(state_size, 32, 32, action_size)\n",
    "        self.target_net = QNet(state_size, 32, 32, action_size)\n",
    "        self.target_net.load_state_dict(self.DQN.state_dict())\n",
    "        self.optimizer = optim.Adam(self.DQN.parameters(), lr=0.001)\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1  # Initial exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.001\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0, 20) > 18  # Explore\n",
    "        else:\n",
    "            q_values = self.DQN(torch.FloatTensor(state))\n",
    "            return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def experience_replay(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(list, zip(*minibatch))\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        q_values = self.DQN(torch.FloatTensor(states))\n",
    "        next_q_values = self.target_net(torch.FloatTensor(next_states)).max(dim=1).values\n",
    "\n",
    "        targets = q_values.clone()\n",
    "        targets[np.arange(batch_size), actions] = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "################# TRAINING ######################\n",
    "def train(env, agent, num_episodes, batch_size):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.experience_replay(batch_size)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.update_target_network()\n",
    "\n",
    "        # Decay exploration rate\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "\n",
    "        # Print episode information\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "################# Autoplay ######################\n",
    "def play(env, agent):\n",
    "    state = env.reset()\n",
    "    agent.epsilon = 0\n",
    "    while True:\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, done = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "################# MAIN ######################\n",
    "save_data = True\n",
    "train_model = False\n",
    "\n",
    "# define the state_size and action_size based on environment\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "if os.path.exists(\"pre_train.pth\") and train_model == False:\n",
    "    # load pretrained weights\n",
    "    checkpoint = torch.load(\"pre_train.pth\")\n",
    "    loaded_model = agent.DQN\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "env = FlappyBirdEnv()\n",
    "batch_size = 15\n",
    "if train_model == True:\n",
    "    train(env, agent, 350, batch_size)\n",
    "else:\n",
    "    for i in range(1,10):\n",
    "        play(env, agent)\n",
    "\n",
    "if save_data == True and train_model == True:\n",
    "    checkpoint_path = 'pre_train.pth'\n",
    "    torch.save({'model_state_dict': agent.DQN.state_dict(),\n",
    "                'target_model_dict': agent.target_net.state_dict(),\n",
    "                'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "                'replay_buffer': list(agent.replay_buffer),\n",
    "                'gamma': agent.gamma,\n",
    "                'epsilon': agent.epsilon,\n",
    "                'epsilon_decay': agent.epsilon_decay\n",
    "                }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model, we tried out several different values for the parameters such as the number of of epochs, number of nodes in the hidden layer, the batch sie, learning rate and clock speed.  Below are the results of the values that we tried.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table, when we tried changing values for the learning rate and batch size, but the average score was drasitically reduced. This meant that these values were very sensitive to the outcome and we decided not to tinker with them much after the first attempt. After increasing the clocksize by a huge amount, we realized that small changes to this parameter would be a better choice. We also found that the sweet spot for the number of epochs was 350, as too many epochs would output a much worse average score. The best results of these test came out from a combination of 350 epochs, 15 batch size, and 0.01 learning rate, with a results of 200+ score. 200+ score in the tables here actually mean that they were able to achieve a state where it could go infinitely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, our model was able to successfully learn to play the game to a level which resulted in an infinite score, however the training process for these kinds of results took quite a lot of time. There are several areas where we think could be improved in the model to enhance the speed and learning rate. First, we think that a convolutional network as the DQ network would allow for the agent to learn in a faster manner, reaching higher scores with less training time. Giving the agent more input parameters could also aid the agent in recognising specific patterns of when to jump, thus effectively enhancing the training the process, but a balance must be struck such that the network isn't overwhelmed with unnecessary inputs that would only hinder the agent from learning.\n",
    "\n",
    "In conclusion, although only one of our three attempted models produced anything, we were happy with the results of the one even if the training took quite a long time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
